# Read in lines from the SOTU text file
sou = readLines("stateoftheunion1790-2012.txt")
# Summary statistics
length(sou)
summary(nchar(sou))
head(sou)

#Q3B - Used regular expressions to find a "***" that comprises a whole line.  
speechbreaks=grep("^[\\*]{3}$",sou)

#Q3C - Extract Date
date = sou[grep("^\\*{3}$", sou)+4]
date = date[grep("[[:digit:]]{4}", date)]

#Q3D - Extract Month
month = substring(date,1,regexpr("[[:alpha:]][[:blank:]][[:digit:]]",date))

#Q3E - Extract year
year = unlist(strsplit(date,","))
year = year[grep("[0-9]{3}",year)]

#Q3F - Extract President
pres = sou[grep("^[//*]{3}$",sou)+3]
pres = pres[grep("[[:alpha:]]$",pres)]


#Q3G - Chop up the speeches. Create a list where each speech is an element within the list 
# and within each element there is a character vector. The character vector contains all of the lines (combined) for each speech. 
# Create a dataframe with start and end points 
strt1 = grep("^[//*]{3}$",sou)+5
strt = strt1[1:214]
end1 = grep("^[//*]{3}$",sou)-2
end = end1[-c(1,216)]
startend = data.frame ("start"=strt1, "end" =end1)

# Create a blank list and then use the paste function to populate this list. 
combination=list()
for (i in 1:length(startend[,1])){
        combination[i] <- paste(sou[startend$start[i]:startend$end[i]], sep=" ", collapse=" " )
}
       
#Q3H - Make each  make each element of the vector a character string corresponding
# to a sentence in the speech Word Vectors
sentences=list()
for (i in 1:length(combination)){
  test = sapply(combination[[i]], strsplit , split = "\\. |\\? ")
  sentences[i]= test
}


#Q3I- Eliminate apostrophes, numbers, and the phrase: (Applause.) from the text.
replace=list()
for (g in 1:length(sentences)){
  replace[[g]]=unlist(lapply(sentences[[g]],gsub, pattern = "[[:digit:]]|[\\']|\\([Aa]pplause\\.*\\)", replace = ""))
}

# Check if this worked
regexpr("\\([Aa]pplause\\.*\\)",sentences)
regexpr("\\([Aa]pplause\\.*\\)",replace)
#Test Case
st = sentences[1:2]
st[[2]][1]
rplc1=list()
for (g in 1:length(sent)){
  rplc1[[g]]=unlist(lapply(sent[[g]],gsub, pattern = "[[:digit:]]|[\\']|\\([Aa]pplause)", replace = ""))
  }
rplc1

#Q3J - Make all of the elements lower case
lower=list()
for (g in 1:length(replace)){
  lower[[g]]=unlist(lapply(replace[[g]],tolower))
}


#Q3K - Split up on punctuation and blanks
punct=list()
for (i in 1:length(lower)){
  punct[[i]] = unlist(lapply(lower[[i]], strsplit , "[[:punct:]]|[[:blank:]]"))
}

#Q3L - Drop Empty Words
full=list()
for (i in 1:length(punct)){
  full[[i]] = punct[[i]][nchar(punct[[i]])>0]
}

#Check to make sure this worked by comparing the before and after lists
minval = function(x){
mv = min(nchar(x));
return(mv)}
test = matrix(1:642, nrow = 214, ncol=3)
for (i in 1:214){
test[i,1] = length(punct[[i]][nchar(punct[[i]])==0])
test[i,2] = length(punct[[i]]) - length(full[[i]])
test[i,3] = test[i,1]==test[i,2]
}


#Q3M - Install Snowball and stem the words
install.packages("SnowballC")
library(SnowballC)

# Stemming our most recent list
steml = list()
for (i in 1:length(full)){
  steml[[i]] = unlist(lapply(full[[i]], wordStem))
}

#Q3N - Create a bag of words which contains all of the unique words used across all speeches
bagwords = sort(unique(unlist(steml)))
bagwords = bagwords[nchar(bagwords)!=0] # The stemmer reduced some words down to "", so I removed them. 


#Q3O - Create a word vector for each speech
# This creates a unique word vector for each speech. It includes unique wordstems only. 
# Empty Matrix
wordv = matrix(seq(1,(length(bagwords)*length(steml)),by=1),
               nrow=length(bagwords), ncol=length(steml),
               dimnames = list(bagwords,1:length(steml)))

# This for loop fills in the above matrix with word frequencies. 
# So, if "a" appears 30 times in speech one, it will have 30 in row 1 column 1. 
# It takes a while to run.
for(j in 1:length(steml)){
  for (i in 1:length(bagwords)){
    wordv[i,j] = length(steml[[j]][steml[[j]]==bagwords[i]])
  }
}


#Q3P-Normalize the word vectors to get term frequencies Analysis
# Create a matrix of normalized term frequencies (fraction of words in the document that are this term)
wordsInDoc = apply(wordv, 2, sum)
normTermFreq = matrix(0, nrow = nrow(wordv), 
                      ncol = ncol(wordv))

for (i in 1:ncol(wordv)) {
  normTermFreq[, i] = wordv[, i]/wordsInDoc[i]
}

# Creates a vector of document frequencies for the next part
dfq = apply(wordv, 1, function(x) sum(x > 0))

#q3Q-  Carry out some exploratory analysis of the data

# I. Number of sentences
# Create aVector with the total number of sentences in each speech.
# This refers to list from Q3H. 
speechsent=sapply(sentences, length)
# 56325 sentences in total
sum(speechsent)

#II. Extract the long words
fullwords = sort(unique(unlist(full))) # Sorted list of word lengths
orderedfullwords = fullwords[order(-nchar(fullwords))] # Ordered list of words by length
orderedfullwords[nchar(orderedfullwords)>=17] # List of words with 17 or more character lists
#[1] "institutionalization" "internationalization" "extraterritoriality"  "unconstitutionality" 
#[5] "characteristically"   "disproportionately"   "intercommunication"   "interparliamentary"  
#[9] "misrepresentations"   "overcapitalization"   "telecommunications"   "ultraconservatives"  
#[13] "commercialization"    "comprehensiveness"    "constitutionality"    "contemporaneously"   
#[17] "disinterestedness"    "disproportionally"    "environmentalists"    "incomprehensively"   
#[21] "indistinguishable"    "instrumentalities"    "interdepartmental"    "intergovernmental"   
#[25] "internationalized"    "interrelationship"    "maladministration"    "misinterpretation"   
#[29] "misrepresentation"    "misunderstandings"    "nondiscrimination"    "nondiscriminatory"   
#[33] "plenipotentiaries"    "superintendencies"    "ultraconservative"   

# III. Find party of each president
# Read in party sheet and add in add names manually
party = read.table("parties.txt") 
partynames=data.frame(V1=c("D","DR","F","R","U","W"),V2=c("Democrat","Democrat-Republican","Federalist","Republican","Union","Whig"))
party$name = partynames$V2[match(party$V1,partynames$V1)]

#Add party and party names to a summary data frame
# pres, date, month and year are from the Q3 C-F
pres.details= data.frame(pres,date, month, year)
pres.details$partyv = party$V1[match(pres.details$pres,party$V2)]
pres.details$partyn = party$name[match(pres.details$pres,party$V2)]
t(t(tapply(pres.details$pres, pres.details$partyn, length)))
#[,1]
#Democrat              76
#Democrat-Republican   28
#Federalist            12
#Republican            86
#Union                  4
#Whig                   8


#IIIB. Find party names mentioned by the President in his speech
allwords=unlist(empt) # Combines all words into a single vector
partyloc=grep("party",allwords) # Finds the location of all words that contains "party"
findparty= data.frame(allwords[partyloc-2], allwords[partyloc-1], allwords[partyloc]) 
# Creates a data frame that contains the words party and the 2 words that precede it. 
unique.party=unique(findparty[findparty[1]=="the",]) 

#2                      the                neutral              party
#4                      the                  other              party
#44                     the                injured              party
#45                     the               opposite              party
#51                     the              aggrieved              party
#53                     the                    one              party
#54                     the                hostile              party
#62                     the               dominant              party
#65                     the              political              party
#67                     the               american              party
#74                     the             vanquished              party
#80                     the              intruding              party
#87                     the            antislavery              party
#88                     the         constitutional              party
#91                     the              insurgent              party
#102                    the               opposing              party
#104                    the          distinguished              party
#123                    the             successful              party
#144                    the                  unfit              party
#151                    the             victorious              party
#152                    the          congressional              party
#158                    the             prevailing              party
#177                    the                adverse              party
#182                    the               stronger              party
#190                    the             republican              party
#200                    the               majority              party
#215                    the                  peace              party
#217                    the             presidents              party
#221                    the              communist              party
#233                    the             opposition              party


#Q3R - Carry out Multidimensional scaling

# Find distance
# COmpute SJDistance is a function. 
computeSJDistance =
  function(tf, df, terms, logdf = TRUE, verbose = TRUE)
  {
    # terms - a character vector of all the unique words, length numTerms
    # df - a numeric vector, length numTerms, number of docs that contains term
    # tf - matrix, numTerms rows, numCols cols (number of documents)
    
    numTerms = nrow(tf)
    numDocs = ncol(tf)
    
    tfn =  t(tf)/colSums(tf)
    if (logdf) tfidfs = t(tfn) * (log(numDocs) - log(df))
    else  tfidfs = numDocs * ( t(tfn) / df)
    
    D.SJ = matrix(0, numDocs, numDocs)
    for(i in 1:(numDocs -1)) {
      for(j in (i+1):numDocs) { 
        D.SJ[i,j] = D.SJ[j,i] = D.JensenShannon(tfidfs[, i], tfidfs[, j])
      }
      if(verbose)
        print(i)  
    }
    return(D.SJ)
  }

D.BasicKullbackLeibler = function(p, q)
{
  tmp = !(p == 0 | q == 0)
  p = p[tmp]
  q = q[tmp]
  
  sum(- p*log(q) + p*log(p) )
}

D.JensenShannon = function(p, q)
{
  T = 0.5 * (p + q)  
  0.5 * D.BasicKullbackLeibler(p, T) + 0.5 * D.BasicKullbackLeibler(q, T)
}  

# This creates a distance matrix for each document
simMatrix = computeSJDistance(tf = wordv, 
                              df = dfq, terms = bagOfWords, 
                              logdf = FALSE)
rownames(simMatrix) = paste(pres,year, sep = "-")
colnames(simMatrix) = paste(pres,year, sep = "-")


#Q3R - Multidimensional Scaling 
mds =cmdscale(simMatrix)
plot(mds, type = "n", xlab = "", ylab = "", main="Documents")
par(cex=0.5)
text(mds, rownames(simMatrix))


# Q3S - Hierarchical Clustering

# Complete Form
par(cex=0.55)
documents = as.dist(simMatrix)
hc = hclust(documents)
plot(hc, main = "Complete Linkage ")


# Single Form
par(cex=0.55)
hc2 = hclust(documents,"single" )
plot(hc2, main = "Single Linkage")
